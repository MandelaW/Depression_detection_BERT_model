{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandem296/Depression_detection_BERT_model/blob/main/BERT_depression_Detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de71870-58e5-4893-ca27-e55ddab074a0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJrQFQgN_BE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "88c1b670-37a7-4c9d-b6cc-9be519db0e9a"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df=pd.read_csv('/content/Mental-Health-Twitter.csv')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-004ed752-95a4-43dd-8ec2-a66ab7f7d7ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-004ed752-95a4-43dd-8ec2-a66ab7f7d7ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Mental-Health-Twitter.csv to Mental-Health-Twitter (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8d20bd-0c84-4d12-ff54-ff4348b6862d"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676DPU1BOPdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b204d4a-76f7-4ecf-ed97-a66c2cc99d09"
      },
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.5\n",
              "0    0.5\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['post_text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed42282-a27c-48dd-9763-b14ee1f33fa6"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zOKeOMeO-DT"
      },
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAH73n39PHLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd79dfa-327b-42e3-8273-23aff067c112"
      },
      "source": [
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c0e9bc08-4e2e-4092-8f1c-6fe1a6240388"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faf2bdbd610>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542cc920-27d7-4368-f8c1-02f18e62c5d2"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1501d280-0b5e-4c71-dab4-15247d4704c9"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373efd4c-a67b-4234-801e-bff48fa20d22"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 200"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      #elapsed = format(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b84e2e9-bbc4-4a01-eeb0-011cdac778bd"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.691\n",
            "Validation Loss: 0.654\n",
            "\n",
            " Epoch 2 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.657\n",
            "Validation Loss: 0.688\n",
            "\n",
            " Epoch 3 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.635\n",
            "Validation Loss: 0.600\n",
            "\n",
            " Epoch 4 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.616\n",
            "Validation Loss: 0.570\n",
            "\n",
            " Epoch 5 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.610\n",
            "Validation Loss: 0.590\n",
            "\n",
            " Epoch 6 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.602\n",
            "Validation Loss: 0.617\n",
            "\n",
            " Epoch 7 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.591\n",
            "Validation Loss: 0.543\n",
            "\n",
            " Epoch 8 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.590\n",
            "Validation Loss: 0.544\n",
            "\n",
            " Epoch 9 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.585\n",
            "Validation Loss: 0.531\n",
            "\n",
            " Epoch 10 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.588\n",
            "Validation Loss: 0.545\n",
            "\n",
            " Epoch 11 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.582\n",
            "Validation Loss: 0.593\n",
            "\n",
            " Epoch 12 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.577\n",
            "Validation Loss: 0.564\n",
            "\n",
            " Epoch 13 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.576\n",
            "Validation Loss: 0.520\n",
            "\n",
            " Epoch 14 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.573\n",
            "Validation Loss: 0.533\n",
            "\n",
            " Epoch 15 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.579\n",
            "Validation Loss: 0.549\n",
            "\n",
            " Epoch 16 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.571\n",
            "Validation Loss: 0.519\n",
            "\n",
            " Epoch 17 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.568\n",
            "Validation Loss: 0.516\n",
            "\n",
            " Epoch 18 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.559\n",
            "Validation Loss: 0.530\n",
            "\n",
            " Epoch 19 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.560\n",
            "Validation Loss: 0.509\n",
            "\n",
            " Epoch 20 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.562\n",
            "Validation Loss: 0.545\n",
            "\n",
            " Epoch 21 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 0.515\n",
            "\n",
            " Epoch 22 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.558\n",
            "Validation Loss: 0.509\n",
            "\n",
            " Epoch 23 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.559\n",
            "Validation Loss: 0.520\n",
            "\n",
            " Epoch 24 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.557\n",
            "Validation Loss: 0.511\n",
            "\n",
            " Epoch 25 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.566\n",
            "Validation Loss: 0.505\n",
            "\n",
            " Epoch 26 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.553\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 27 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.555\n",
            "Validation Loss: 0.512\n",
            "\n",
            " Epoch 28 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.549\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 29 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.548\n",
            "Validation Loss: 0.505\n",
            "\n",
            " Epoch 30 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.556\n",
            "Validation Loss: 0.523\n",
            "\n",
            " Epoch 31 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.557\n",
            "Validation Loss: 0.501\n",
            "\n",
            " Epoch 32 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.551\n",
            "Validation Loss: 0.538\n",
            "\n",
            " Epoch 33 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.549\n",
            "Validation Loss: 0.539\n",
            "\n",
            " Epoch 34 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.548\n",
            "Validation Loss: 0.512\n",
            "\n",
            " Epoch 35 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.545\n",
            "Validation Loss: 0.490\n",
            "\n",
            " Epoch 36 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.536\n",
            "Validation Loss: 0.506\n",
            "\n",
            " Epoch 37 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.551\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 38 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.554\n",
            "Validation Loss: 0.499\n",
            "\n",
            " Epoch 39 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.541\n",
            "Validation Loss: 0.497\n",
            "\n",
            " Epoch 40 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.547\n",
            "Validation Loss: 0.542\n",
            "\n",
            " Epoch 41 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.541\n",
            "Validation Loss: 0.498\n",
            "\n",
            " Epoch 42 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.543\n",
            "Validation Loss: 0.504\n",
            "\n",
            " Epoch 43 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.537\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 44 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.534\n",
            "Validation Loss: 0.481\n",
            "\n",
            " Epoch 45 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.540\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 46 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.538\n",
            "Validation Loss: 0.509\n",
            "\n",
            " Epoch 47 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.542\n",
            "Validation Loss: 0.494\n",
            "\n",
            " Epoch 48 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.538\n",
            "Validation Loss: 0.495\n",
            "\n",
            " Epoch 49 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.541\n",
            "Validation Loss: 0.514\n",
            "\n",
            " Epoch 50 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.535\n",
            "Validation Loss: 0.505\n",
            "\n",
            " Epoch 51 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.542\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 52 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.546\n",
            "Validation Loss: 0.495\n",
            "\n",
            " Epoch 53 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.536\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 54 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.537\n",
            "Validation Loss: 0.484\n",
            "\n",
            " Epoch 55 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.532\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 56 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.539\n",
            "Validation Loss: 0.481\n",
            "\n",
            " Epoch 57 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.532\n",
            "Validation Loss: 0.489\n",
            "\n",
            " Epoch 58 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.534\n",
            "Validation Loss: 0.492\n",
            "\n",
            " Epoch 59 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.532\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 60 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.531\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 61 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.535\n",
            "Validation Loss: 0.479\n",
            "\n",
            " Epoch 62 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.527\n",
            "Validation Loss: 0.519\n",
            "\n",
            " Epoch 63 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.531\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 64 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.533\n",
            "Validation Loss: 0.485\n",
            "\n",
            " Epoch 65 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.528\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 66 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.486\n",
            "\n",
            " Epoch 67 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.532\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 68 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.523\n",
            "Validation Loss: 0.496\n",
            "\n",
            " Epoch 69 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.533\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 70 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.528\n",
            "Validation Loss: 0.484\n",
            "\n",
            " Epoch 71 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.528\n",
            "Validation Loss: 0.539\n",
            "\n",
            " Epoch 72 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.529\n",
            "Validation Loss: 0.478\n",
            "\n",
            " Epoch 73 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.525\n",
            "Validation Loss: 0.501\n",
            "\n",
            " Epoch 74 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.523\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 75 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.527\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 76 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.524\n",
            "Validation Loss: 0.487\n",
            "\n",
            " Epoch 77 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.528\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 78 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 79 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 80 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.518\n",
            "Validation Loss: 0.483\n",
            "\n",
            " Epoch 81 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 82 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.524\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 83 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.521\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 84 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.525\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 85 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.523\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 86 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.527\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 87 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.523\n",
            "Validation Loss: 0.483\n",
            "\n",
            " Epoch 88 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.522\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 89 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.522\n",
            "Validation Loss: 0.491\n",
            "\n",
            " Epoch 90 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.521\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 91 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.481\n",
            "\n",
            " Epoch 92 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.522\n",
            "Validation Loss: 0.465\n",
            "\n",
            " Epoch 93 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.521\n",
            "Validation Loss: 0.523\n",
            "\n",
            " Epoch 94 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 95 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.506\n",
            "\n",
            " Epoch 96 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.522\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 97 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.475\n",
            "\n",
            " Epoch 98 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.516\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 99 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.518\n",
            "Validation Loss: 0.465\n",
            "\n",
            " Epoch 100 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.517\n",
            "Validation Loss: 0.466\n",
            "\n",
            " Epoch 101 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.513\n",
            "Validation Loss: 0.550\n",
            "\n",
            " Epoch 102 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.475\n",
            "\n",
            " Epoch 103 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 104 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.521\n",
            "Validation Loss: 0.507\n",
            "\n",
            " Epoch 105 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.516\n",
            "Validation Loss: 0.478\n",
            "\n",
            " Epoch 106 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.514\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 107 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.515\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 108 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 109 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.514\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 110 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.517\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 111 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.518\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 112 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.513\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 113 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 114 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.516\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 115 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.515\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 116 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.517\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 117 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.487\n",
            "\n",
            " Epoch 118 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.457\n",
            "\n",
            " Epoch 119 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.517\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 120 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.518\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 121 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.513\n",
            "Validation Loss: 0.459\n",
            "\n",
            " Epoch 122 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.467\n",
            "\n",
            " Epoch 123 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.463\n",
            "\n",
            " Epoch 124 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.465\n",
            "\n",
            " Epoch 125 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 126 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.512\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 127 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.509\n",
            "Validation Loss: 0.483\n",
            "\n",
            " Epoch 128 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.514\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 129 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.454\n",
            "\n",
            " Epoch 130 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.515\n",
            "Validation Loss: 0.458\n",
            "\n",
            " Epoch 131 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.481\n",
            "\n",
            " Epoch 132 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 133 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.516\n",
            "Validation Loss: 0.467\n",
            "\n",
            " Epoch 134 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 135 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 136 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.514\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 137 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 138 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.495\n",
            "\n",
            " Epoch 139 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.513\n",
            "Validation Loss: 0.460\n",
            "\n",
            " Epoch 140 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.512\n",
            "Validation Loss: 0.484\n",
            "\n",
            " Epoch 141 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.515\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 142 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 143 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 144 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.456\n",
            "\n",
            " Epoch 145 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 146 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 147 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 148 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.475\n",
            "\n",
            " Epoch 149 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 150 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 151 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.472\n",
            "\n",
            " Epoch 152 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 153 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 154 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.454\n",
            "\n",
            " Epoch 155 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 156 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 157 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.504\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 158 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.463\n",
            "\n",
            " Epoch 159 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 160 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 161 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 162 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.484\n",
            "\n",
            " Epoch 163 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.500\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 164 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 165 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 166 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.506\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 167 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.504\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 168 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 169 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.457\n",
            "\n",
            " Epoch 170 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.460\n",
            "\n",
            " Epoch 171 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.487\n",
            "\n",
            " Epoch 172 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 173 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.507\n",
            "Validation Loss: 0.457\n",
            "\n",
            " Epoch 174 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.496\n",
            "Validation Loss: 0.466\n",
            "\n",
            " Epoch 175 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.504\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 176 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 177 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 178 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 179 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 180 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 181 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.456\n",
            "\n",
            " Epoch 182 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.521\n",
            "\n",
            " Epoch 183 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 184 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 185 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.496\n",
            "Validation Loss: 0.454\n",
            "\n",
            " Epoch 186 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.504\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 187 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 188 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.452\n",
            "\n",
            " Epoch 189 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 190 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.500\n",
            "Validation Loss: 0.466\n",
            "\n",
            " Epoch 191 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.498\n",
            "Validation Loss: 0.447\n",
            "\n",
            " Epoch 192 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 193 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.501\n",
            "Validation Loss: 0.460\n",
            "\n",
            " Epoch 194 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.478\n",
            "\n",
            " Epoch 195 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.497\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 196 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.467\n",
            "\n",
            " Epoch 197 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.496\n",
            "Validation Loss: 0.470\n",
            "\n",
            " Epoch 198 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.502\n",
            "Validation Loss: 0.446\n",
            "\n",
            " Epoch 199 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.471\n",
            "\n",
            " Epoch 200 / 200\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     94.\n",
            "\n",
            "Training Loss: 0.495\n",
            "Validation Loss: 0.468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cd9f1c-b8a6-42b2-fc8e-dd5a2aeb32d5"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea90eda7-d667-4d9d-a801-4598d6f27273"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.71      0.76      1500\n",
            "           1       0.74      0.84      0.79      1500\n",
            "\n",
            "    accuracy                           0.78      3000\n",
            "   macro avg       0.78      0.78      0.77      3000\n",
            "weighted avg       0.78      0.78      0.77      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a074c638-8657-4128-ae20-50106a94c56e"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "col_0     0     1\n",
              "row_0            \n",
              "0      1058   442\n",
              "1       233  1267"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9174a505-11d0-480d-9412-857aa2a6ce2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1058</td>\n",
              "      <td>442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>233</td>\n",
              "      <td>1267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9174a505-11d0-480d-9412-857aa2a6ce2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9174a505-11d0-480d-9412-857aa2a6ce2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9174a505-11d0-480d-9412-857aa2a6ce2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}